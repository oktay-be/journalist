diff --git a/src/journalist/core/file_manager.py b/src/journalist/core/file_manager.py
index 2ca5664..c2d6641 100644
--- a/src/journalist/core/file_manager.py
+++ b/src/journalist/core/file_manager.py
@@ -6,7 +6,7 @@ import os
 import json
 import logging
 from datetime import datetime, timedelta
-from typing import Dict, Optional, Any
+from typing import Dict, Optional, Any, List
 from werkzeug.utils import secure_filename
 import datefinder
 
@@ -424,3 +424,138 @@ class FileManager:
             logger.warning(f"Error checking article age for {url}: {e}")
             # On error, don't filter the article
             return False
+
+    def _get_source_session_filename(self, domain: str) -> str:
+        """
+        Generate session filename for specific domain, reusing existing sanitization.
+        
+        Args:
+            domain: Domain name like 'www.fanatik.com.tr'
+            
+        Returns:
+            Filename like 'session_data_fanatik_com_tr.json'
+        """
+        # Remove www prefix and replace dots/dashes with underscores
+        sanitized_domain = domain.replace('www.', '').replace('.', '_').replace('-', '_').replace(':', '_')
+        # Use existing sanitization method
+        safe_domain = self._sanitize_filename(sanitized_domain)
+        return f"session_data_{safe_domain}.json"
+
+    def save_source_specific_session_data(self, domain: str, session_data: Dict[str, Any]) -> bool:
+        """
+        Save session data for specific source domain.
+        
+        Args:
+            domain: Source domain name
+            session_data: Session data dictionary to save
+            
+        Returns:
+            True if saved successfully, False otherwise
+        """
+        try:
+            filename = self._get_source_session_filename(domain)
+            file_path = os.path.join(self.base_data_dir, filename)
+            
+            # Add domain metadata if not already present
+            if 'source_domain' not in session_data:
+                session_data['source_domain'] = domain
+            
+            # Add timestamp if not already present
+            if 'saved_at' not in session_data:
+                session_data['saved_at'] = datetime.now().isoformat()
+            
+            return self.save_json_data(file_path, session_data, data_type=f"source session ({domain})")
+            
+        except Exception as e:
+            logger.error(f"Failed to save source-specific session data for domain {domain}: {e}")
+            return False
+
+    def load_source_specific_session_data(self, domain: str) -> Optional[Dict[str, Any]]:
+        """
+        Load session data for specific source domain.
+        
+        Args:
+            domain: Source domain name
+            
+        Returns:
+            Session data dictionary or None if not found
+        """
+        try:
+            filename = self._get_source_session_filename(domain)
+            file_path = os.path.join(self.base_data_dir, filename)
+            return self.load_json_data(file_path, data_type=f"source session ({domain})")
+            
+        except Exception as e:
+            logger.error(f"Failed to load source-specific session data for domain {domain}: {e}")
+            return None
+
+    def list_source_session_files(self) -> List[str]:
+        """
+        List all source-specific session data files in the session directory.
+        
+        Returns:
+            List of source session filenames (not full paths)
+        """
+        try:
+            if not os.path.exists(self.base_data_dir):
+                return []
+            
+            files = []
+            for filename in os.listdir(self.base_data_dir):
+                if filename.startswith('session_data_') and filename.endswith('.json') and filename != 'session_data.json':
+                    files.append(filename)
+            
+            return sorted(files)  # Sort for consistent ordering
+            
+        except Exception as e:
+            logger.error(f"Error listing source session files: {e}")
+            return []
+
+    def save_individual_articles(self, articles: List[Dict[str, Any]]) -> None:
+        """
+        Save individual articles to files.
+        
+        Args:
+            articles: List of article dictionaries to save
+        """
+        try:
+            for i, article in enumerate(articles):
+                article_url = article.get('url', '')
+                if article_url:
+                    # Use URL-based filename
+                    self.save_article_by_url(
+                        url=article_url,
+                        article_data=article,
+                        counter=i,
+                        include_html_content=False
+                    )
+                else:
+                    # Fallback to old method if no URL
+                    article_id = article.get('id') or f"article_{i}"
+                    self.save_article(article_id, article, include_html_content=False)
+        except Exception as e:
+            logger.error(f"Error saving individual articles: {e}")
+
+    def save_source_session_files(self, source_session_data_list: List[Dict[str, Any]]) -> List[str]:
+        """
+        Save source-specific session data files.
+        
+        Args:
+            source_session_data_list: List of source session data to save
+            
+        Returns:
+            List of saved filenames
+        """
+        saved_files = []
+        try:
+            for source_data in source_session_data_list:
+                domain = source_data.get('source_domain', 'unknown')
+                success = self.save_source_specific_session_data(domain, source_data)
+                if success:
+                    filename = self._get_source_session_filename(domain)
+                    saved_files.append(filename)
+                    
+            return saved_files
+        except Exception as e:
+            logger.error(f"Error saving source session files: {e}")
+            return saved_files
diff --git a/src/journalist/core/web_scraper.py b/src/journalist/core/web_scraper.py
index 444d4f9..c4889e7 100644
--- a/src/journalist/core/web_scraper.py
+++ b/src/journalist/core/web_scraper.py
@@ -293,4 +293,42 @@ class WebScraper:
     
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         """Async context manager exit"""
-        await self.session_manager.__aexit__(exc_type, exc_val, exc_tb)
\ No newline at end of file
+        await self.session_manager.__aexit__(exc_type, exc_val, exc_tb)
+
+    def create_source_session_data(self, grouped_sources: Dict[str, Dict[str, Any]], session_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
+        """
+        Create source-specific session data structures.
+        
+        Args:
+            grouped_sources: Articles grouped by source domain
+            session_metadata: Metadata from scraping session
+            
+        Returns:
+            List of source-specific session data dictionaries
+        """
+        try:
+            source_session_data_list = []
+            
+            for domain, source_data in grouped_sources.items():
+                # Create source-specific session data
+                source_session_item = {
+                    'source_domain': source_data['source_domain'],
+                    'source_url': source_data['source_url'],
+                    'articles': source_data['articles'],
+                    'articles_count': source_data['articles_count'],
+                    'saved_at': datetime.now().isoformat(),
+                    'session_metadata': {
+                        **session_metadata,
+                        'source_specific': True,
+                        'source_domain': domain,
+                        'articles_scraped': source_data['articles_count']
+                    }
+                }
+                
+                source_session_data_list.append(source_session_item)
+            
+            return source_session_data_list
+            
+        except Exception as e:
+            logger.error(f"Error creating source session data: {e}")
+            return []
\ No newline at end of file
diff --git a/src/journalist/journalist.py b/src/journalist/journalist.py
index 91c5ff4..f1af49c 100644
--- a/src/journalist/journalist.py
+++ b/src/journalist/journalist.py
@@ -142,6 +142,124 @@ class Journalist:
         
         return filtered_articles
 
+    def _group_articles_by_source(self, articles: List[Dict[str, Any]], original_urls: List[str]) -> Dict[str, Dict[str, Any]]:
+        """
+        Group articles by their source domain, reusing existing get_domain utility.
+        
+        Args:
+            articles: List of article dictionaries
+            original_urls: List of original URLs provided by user
+            
+        Returns:
+            Dictionary mapping domain names to source data with articles
+        """
+        try:
+            # Import here to avoid circular imports
+            from .core.network_utils import get_domain
+            
+            grouped_sources = {}
+            
+            for article in articles:
+                # Get article URL
+                article_url = article.get('url', '')
+                
+                if not article_url:
+                    # If no URL, try to find a matching domain from original URLs
+                    if original_urls:
+                        domain = get_domain(original_urls[0])
+                    else:
+                        domain = 'unknown'
+                else:
+                    domain = get_domain(article_url)
+                
+                if not domain:
+                    domain = 'unknown'
+                
+                # Initialize domain group if not exists
+                if domain not in grouped_sources:
+                    # Find the original URL for this domain
+                    source_url = domain
+                    for orig_url in original_urls:
+                        if get_domain(orig_url) == domain:
+                            source_url = orig_url
+                            break
+                    
+                    grouped_sources[domain] = {
+                        'source_domain': domain,
+                        'source_url': source_url,
+                        'articles': [],
+                        'articles_count': 0
+                    }
+               
+                # Add article to appropriate domain group
+                grouped_sources[domain]['articles'].append(article)
+                grouped_sources[domain]['articles_count'] += 1
+            
+            # Log grouping results
+            for domain, source_data in grouped_sources.items():
+                logger.info(f"Grouped {source_data['articles_count']} articles for domain: {domain}")
+            
+            return grouped_sources
+            
+        except Exception as e:
+            logger.error(f"Error grouping articles by source: {e}")
+            # Fallback: put all articles under 'unknown' domain
+            return {
+                'unknown': {
+                    'source_domain': 'unknown',
+                    'source_url': 'unknown',
+                    'articles': articles,
+                    'articles_count': len(articles)                }
+            }
+
+    def process_articles(
+        self, 
+        scraped_articles: List[Dict[str, Any]], 
+        original_urls: List[str],
+        session_metadata: Dict[str, Any]
+    ) -> List[Dict[str, Any]]:
+        """
+        Process articles with filtering, saving, and source segregation.
+        
+        Always returns the same data structure regardless of persistence mode.
+        
+        Args:
+            scraped_articles: Raw articles from web scraper
+            original_urls: Original URLs provided by user for source identification
+            session_metadata: Metadata from scraping session
+            
+        Returns:
+            List of source-specific session data dictionaries
+        """
+        try:
+            # 1. Filter articles by date
+            filtered_articles = self._filter_articles_by_date(scraped_articles)
+            
+            # 2. Group articles by source
+            grouped_sources = self._group_articles_by_source(filtered_articles, original_urls)
+            
+            # 3. Create source-specific session data (delegated to WebScraper)
+            source_session_data_list = self.web_scraper.create_source_session_data(grouped_sources, session_metadata)
+            
+            # 4. Handle persistence (if enabled)
+            if self.persist and self.file_manager:
+                # Save individual articles (delegated to FileManager)
+                self.file_manager.save_individual_articles(filtered_articles)
+                
+                # Save source-specific session data files (delegated to FileManager)
+                saved_files = self.file_manager.save_source_session_files(source_session_data_list)
+                logger.info(f"Saved {len(saved_files)} source session files: {saved_files}")
+            else:
+                # Store in memory for non-persistent mode
+                self.memory_articles.extend(filtered_articles)
+            
+            # 5. Always return the same structure
+            return source_session_data_list
+            
+        except Exception as e:
+            logger.error(f"Error processing articles: {e}")
+            return []
+
     async def read(self, urls: List[str], keywords: Optional[List[str]] = None) -> Dict[str, Any]:
         """
         Extract content from the provided URLs with optional keyword filtering.
@@ -218,70 +336,28 @@ class Journalist:
                     if task_type == 'web_scrape':                        # New modular WebScraper returns session data with articles and metadata
                         if result and isinstance(result, dict):                            # Extract articles from the session data
                             scraped_articles = result.get('articles', [])
-                            
-                            if self.persist and self.file_manager:
-                                # Filter articles by date before saving
-                                filtered_articles = self._filter_articles_by_date(scraped_articles)
-                                
-                                # Save individual articles using FileManager with URL-based filenames
-                                for i, article in enumerate(filtered_articles):
-                                    article_url = article.get('url', '')
-                                    if article_url:
-                                        # Use URL-based filename
-                                        self.file_manager.save_article_by_url(
-                                            url=article_url,
-                                            article_data=article,
-                                            counter=i,
-                                            include_html_content=False
-                                        )
-                                    else:
-                                        # Fallback to old method if no URL
-                                        article_id = article.get('id') or f"article_{i}_{self.session_id}"
-                                        self.file_manager.save_article(article_id, article, include_html_content=False)
-                                  # Save session metadata with filtered articles
-                                session_file = os.path.join(self.file_manager.base_data_dir, "session_data.json")
-                                # Create a new result with filtered articles
-                                filtered_result = result.copy()
-                                filtered_result['articles'] = filtered_articles
-                                session_payload = {
-                                    'saved_at': datetime.now().isoformat(),
-                                    'articles_count': len(filtered_articles),
-                                    **filtered_result
-                                }
-                                self.file_manager.save_json_data(session_file, session_payload, data_type="session")
-                                
-                                # Use filtered articles for final result
-                                scraped_articles = filtered_articles
-                            else:
-                                # Filter articles by date for memory mode too
-                                filtered_articles = self._filter_articles_by_date(scraped_articles)
-                                
-                                # Store articles in memory for non-persistent mode
-                                self.memory_articles.extend(filtered_articles)
-                                
-                                # Use filtered articles for final result
-                                scraped_articles = filtered_articles
+                              # Filter articles by date
+                            filtered_articles = self._filter_articles_by_date(scraped_articles)
                             
                             # Add articles to our result list
-                            articles.extend(scraped_articles)
+                            articles.extend(filtered_articles)
                         
                         logger.info("Session [%s]: Web scraping complete. Found %d scraped articles.", session_id, len(articles))
         else:
-            logger.info("Session [%s]: No tasks to execute (no URLs provided).", session_id)
-
-        # Prepare final result
-        result = {
-            'articles': articles,
+            logger.info("Session [%s]: No tasks to execute (no URLs provided).", session_id)        # Create session metadata
+        session_metadata = {
             'session_id': session_id,
-            'extraction_summary': {
-                'session_id': session_id,
-                'urls_requested': len(urls),
-                'urls_processed': len(scrape_urls_for_session),
-                'articles_extracted': len(articles),
-                'extraction_time_seconds': round(time.time() - start_time, 2),
-                'keywords_used': keywords or []
-            }
-        }
+            'urls_requested': len(urls),
+            'urls_processed': len(scrape_urls_for_session),
+            'articles_extracted': len(articles),
+            'extraction_time_seconds': round(time.time() - start_time, 2),
+            'keywords_used': keywords or [],
+            'scrape_depth': self.scrape_depth,
+            'persist_mode': self.persist,
+            'extraction_timestamp': datetime.now().isoformat()        }
+
+        # Process articles using new source-specific approach
+        result = self.process_articles(articles, urls, session_metadata)
         
         logger.info("Session [%s]: Extraction completed in %.2f seconds", session_id, time.time() - start_time)
         
